"""
Load training data.
"""

import os
import random
import shutil
import pickle
import collections
from dataclasses import dataclass
from operator import itemgetter
from functools import partial
from typing import Optional, Any
from abc import ABC, abstractmethod

import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset
import numpy as np
import gin

from amago.hindsight import Trajectory, Relabeler, FrozenTraj, NoOpRelabeler


@dataclass
class RLData:
    """
    Stores an individual training example (sequence).

    Args:
        obs (dict[str, torch.Tensor]): A dictionary of observation tensors.
            The shape of each value is (T, Any), where T is the length of the sequence in timesteps.
        rews (torch.FloatTensor): A tensor of rewards. Shape: (T - 1, 1).
            There is no reward before the first timestep.
        dones (torch.BoolTensor): A tensor of done flags. Shape: (T - 1, 1).
            There is no done flag before the first timestep.
        actions (torch.FloatTensor): A tensor of actions. Shape: (T - 1, action dim).
            There is no action on the last (terminal) timestep.
        time_idxs (torch.LongTensor): A tensor of time indices. Shape: (T, 1).
            The global timestep counter of each timestep in the episode.
            If this is an entire episode, `time_idxs` is range(0, T).
        rl2s (Optional[torch.FloatTensor]): A tensor of previous action/reward pairs.
            Shape: (T, 1 + action dim). The previous reward concatenated with the previous action.
            Used to make these values input to the policy.
            If not provided, they will be generated by shifting the actions and rewards.
    """

    obs: dict[str, torch.Tensor]  # (T, obs dim)
    rews: torch.FloatTensor  # (T - 1, 1)
    dones: torch.BoolTensor  # (T - 1, 1)
    actions: torch.FloatTensor  # (T - 1, action dim)
    time_idxs: torch.LongTensor  # (T, 1)
    rl2s: Optional[torch.FloatTensor] = None  # (T, 1 + action dim)

    def __post_init__(self):
        if self.rl2s is None:
            """
            Makes the `rl2s` (previous action/reward) sequence optional, because
            any dataset that isn't generated by our env wrappers wouldn't have it.
            Must always mirror the format of `hindsight.Trajectory.as_input_sequence`.
            """
            blank_action = torch.zeros((1, self.actions.shape[-1]), dtype=torch.float32)
            blank_rew = torch.zeros((1, 1), dtype=torch.float32)
            self.rl2s = torch.cat(
                (
                    torch.cat((blank_rew, self.rews), dim=0),
                    torch.cat((blank_action, self.actions), dim=0),
                ),
                dim=-1,
            )

    def __len__(self):
        return len(self.actions)

    def _safe_randrange(self, low, high):
        return random.randrange(low, max(high, low + 1))

    def random_slice(self, length: int, padded_sampling: str = "none"):
        """
        Randomly slices the sequence to a given length.

        Args:
            length: The length of the sequence to sample.
            padded_sampling: The mode of padding to use.
                "none" --> sample a random start index and take the next `length` timesteps. Can
                bias against the timesteps at the end of the trajectory.
                "both" --> sample without bias against the start and end of the trajectory, but can lead to
                    much more of the batch being padded/masked.
                "left" --> sample while effectively padding the left side of the sequence for cases
                where the start of the training sequence is not always the first timestep of the trajectory.
                "right" --> sample while effectively padding the right side of the sequence for cases
                where the end of the trajectory may be undersampled.
        """
        if len(self) <= length:
            start = 0
        elif padded_sampling == "none":
            start = self._safe_randrange(0, len(self) - length + 1)
        elif padded_sampling == "both":
            start = self._safe_randrange(-length + 1, len(self) - 1)
        elif padded_sampling == "left":
            start = self._safe_randrange(-length + 1, len(self) - length + 1)
        elif padded_sampling == "right":
            start = self._safe_randrange(0, len(self) - 1)
        else:
            raise ValueError(
                f"Unrecognized `padded_sampling` mode: `{padded_sampling}`"
            )
        stop = start + length
        start = max(start, 0)
        # the causal RL loss requires these off-by-one lengths
        tcp = slice(start, stop + 1)
        tc = slice(start, stop)
        self.obs = {k: v[tcp] for k, v in self.obs.items()}
        self.rl2s = self.rl2s[tcp]
        self.time_idxs = self.time_idxs[tcp]
        self.dones = self.dones[tc]
        self.rews = self.rews[tc]
        self.actions = self.actions[tc]
        return self


MAGIC_PAD_VAL = 4.0
pad = partial(pad_sequence, batch_first=True, padding_value=MAGIC_PAD_VAL)


@dataclass
class Batch:
    """
    Keeps data organized during training step
    """

    obs: dict[torch.Tensor]
    rl2s: torch.Tensor
    rews: torch.Tensor
    dones: torch.Tensor
    actions: torch.Tensor
    time_idxs: torch.Tensor

    def to(self, device):
        self.obs = {k: v.to(device) for k, v in self.obs.items()}
        self.rl2s = self.rl2s.to(device)
        self.rews = self.rews.to(device)
        self.dones = self.dones.to(device)
        self.actions = self.actions.to(device)
        self.time_idxs = self.time_idxs.to(device)
        return self


def RLData_pad_collate(samples: list[RLData]) -> Batch:
    """Pads the sequence data of a list of `RLData` gathered by dataloaders.

    Args:
        samples: A list of `RLData` objects.

    Returns:
        A `Batch` object.
    """
    assert samples[0].obs.keys() == samples[-1].obs.keys()
    obs = {k: pad([s.obs[k] for s in samples]) for k in samples[0].obs.keys()}
    rl2s = pad([s.rl2s for s in samples])
    rews = pad([s.rews for s in samples])
    dones = pad([s.dones for s in samples])
    actions = pad([s.actions for s in samples])
    time_idxs = pad([s.time_idxs for s in samples])
    return Batch(
        obs=obs,
        rl2s=rl2s,
        rews=rews,
        dones=dones,
        actions=actions,
        time_idxs=time_idxs,
    )


class RLDataset(ABC, Dataset):
    """Abstract base class that defines what :py:class:`~amago.experiment.Experiment` expects from its pytorch Dataset.

    Allows the main training loop to be used with existing data sources (offline RL)
    that were not generated by our environment wrappers.

    RLDatasets have no real length or fixed indices like typical pytorch Datasets.
    The "length" is determined by the update : data ratio of the experiment.

    Datasets have a reference to the `Experiment` that is training on them, which can be used
    to access the `Accelerator` and other useful information.

    Args:
        dset_name: The name of the dataset. Used to identify the dataset in logging metrics. Defaults to the class name.
    """

    def __init__(self, dset_name: Optional[str] = None):
        self.configured = False
        self.dset_name = dset_name or self.__class__.__name__

    def configure(
        self,
        items_per_epoch: int,
        max_seq_len: int,
        padded_sampling: str,
        has_edit_rights: bool,
    ):
        """Configure the dataset with additional hyperparameters.

        We've basically split the constructor in two parts, because in practice
        we will be grabbing these settings from the main Experiment.

        Args:
            items_per_epoch: The number of trajectoires to sample each epoch. Defines
                the end of dataloader iteration.
            max_seq_len: The maximum sequence length to sample.
            padded_sampling. Strategy for sampling from trajectories longer than max_seq_len.
                See :py:meth:`~amago.loading.RLData.random_slice` or :py:class:`~amago.experiment.Experiment`
                docs for more details.
            has_edit_rights: Whether the dataset has total authority to edit the underlying data
                (e.g., to delete trajectory files from disk). Sometimes turned off for async
                setups.
        """
        self.items_per_epoch = items_per_epoch
        self.max_seq_len = max_seq_len
        self.padded_sampling = padded_sampling
        self.has_edit_rights = has_edit_rights
        self.configured = True

    def configure_from_experiment(self, experiment):
        """Call ``configure()`` with settings inferred from the main Experiment."""
        # convert from length of epoch in batches to length of epoch in items to load
        items_per_epoch = (
            experiment.train_batches_per_epoch
            * experiment.batch_size
            * experiment.accelerator.num_processes
        )
        self.configure(
            items_per_epoch=items_per_epoch,
            max_seq_len=experiment.max_seq_len,
            padded_sampling=experiment.padded_sampling,
            has_edit_rights=experiment.has_dset_edit_rights,
        )

    def check_configured(self):
        if not self.configured:
            raise ValueError(
                "Dataset not configured. Call `configure_from_experiment()` first."
            )

    def __len__(self):
        # the length is set by the number of items we want to sample between
        # env interactions, not by the actual number of items in the dataset.
        self.check_configured()
        return self.items_per_epoch

    @property
    @abstractmethod
    def save_new_trajs_to(self) -> Optional[str]:
        """
        Tells the parallel actors where to write new trajectories.

        If `None`, the actors will not write new trajectories to disk.
        """
        raise NotImplementedError

    @property
    def ready_for_training(self) -> bool:
        """
        Whether the dataset is ready to be used for training.
        If `False`, :py:class:`~amago.experiment.Experiment`
        will keep collecting data until learning updates can begin.
        """
        return self.configured

    def on_end_of_collection(self, experiment) -> dict[str, Any]:
        """
        Callback for :py:class:`~amago.experiment.Experiment` to call
        *after* each round of environment interaction / data collection,
        but *before* gradient updates resume.

        Args:
            epoch: The current epoch (in case there is some kind of schedule)

        Returns a dictionary of metrics to log.
        """
        return {}

    def delete(self):
        """
        Called when the user wants to delete the dataset (from disk?).
        """
        pass

    @abstractmethod
    def get_description(self) -> str:
        """
        Returns a string describing the dataset. Printed as part of the hparam
        summary when the experiment starts.
        """
        raise NotImplementedError

    @abstractmethod
    def sample_random_trajectory(self) -> RLData:
        """
        Prepare a (random) *full trajectory* or at least the maximum sequence length
        of the dataset. Sampling random subsets up to the policy's sequence length is
        handled automatically.
        """
        raise NotImplementedError

    def __getitem__(self, i):
        self.check_configured()
        data = self.sample_random_trajectory()
        if self.max_seq_len is not None:
            data = data.random_slice(
                length=self.max_seq_len, padded_sampling=self.padded_sampling
            )
        return data


@dataclass
class _DatasetStatus:
    dataset: RLDataset
    initial_weight: float
    final_weight: float
    epoch_ready: Optional[int]


class MixtureOfDatasets(RLDataset):
    """Sample from a mixture of datasets.

    Args:
        datasets: A list of :py:class:`~amago.loading.RLDataset` objects.
        sampling_weights: Probability of sampling from each dataset. Must sum to 1.
        smooth_sudden_starts: When a dataset becomes ready for training mid-way through training,
            anneal its sampling_weight from 0 --> assigned ``sampling_weights[i]`` over ``smooth_sudden_starts`` epochs.
        dset_name: The name of the dataset. Used to identify the dataset in logging metrics. Defaults to the class name.

    Note:
        Only samples from datasets that are currently :py:meth:`~amago.loading.RLDataset.ready_for_training`. For example,
        a mixture of some custom offline dataset and the :py:class:`~amago.loading.DiskTrajDataset` will only sample
        from the offline dataset until the :py:class:`~amago.loading.DiskTrajDataset` has written trajectories to disk.
        Sampling weights are renormalized amongst the datasets that are ready for training.

        Only one dataset can direct the Experiment's parallel actors to save new trajectories to disk
        (only one dataset's `dset.save_new_trajs_to` is not `None`).
    """

    def __init__(
        self,
        datasets: list[RLDataset],
        sampling_weights: list[float],
        smooth_sudden_starts: Optional[int] = None,
        dset_name: Optional[str] = None,
    ):
        super().__init__(dset_name=dset_name)
        if not len(datasets) == len(sampling_weights):
            raise ValueError(
                f"{self.__class__.__name__} requires the same number of datasets ({len(datasets)}) as sampling weights ({len(sampling_weights)})"
            )
        if not np.isclose(sum(sampling_weights), 1):
            raise ValueError(
                f"{self.__class__.__name__} requires the sum of sampling weights to be 1.0"
            )
        self.all_datasets = datasets
        self.sampling_weights = sampling_weights
        self.smooth_sudden_starts = smooth_sudden_starts

        # find up to 1 dataset that determines where Experiment will write new trajectories
        active_save_locs = [
            d.save_new_trajs_to
            for d in self.all_datasets
            if d.save_new_trajs_to is not None
        ]
        if len(active_save_locs) > 1:
            raise ValueError("Only one dataset can save new trajectories")
        self._save_new_trajs_to = active_save_locs[0] if active_save_locs else None

    def configure_from_experiment(self, experiment):
        super().configure_from_experiment(experiment)
        for d in self.all_datasets:
            d.configure_from_experiment(experiment)

        # with datasets configured, initialize our annealing schedule
        self._dsets_status = []
        for d, w in zip(self.all_datasets, self.sampling_weights):
            # if the dataset is available from the start of training, turn off its schedule
            # setting initial_weight to the final weight.
            initial_weight = w if d.ready_for_training else 0
            self._dsets_status.append(
                _DatasetStatus(
                    dataset=d,
                    initial_weight=initial_weight,
                    final_weight=w,
                    epoch_ready=None,
                )
            )
        self.update_dset_weights(0)
        # will hold an empirical counter of sampler per dataset per epoch as a wandb sanity-check
        # (only works when dataloader has 0 workers)
        self._sampling_metrics = collections.defaultdict(int)

    @property
    def save_new_trajs_to(self) -> Optional[str]:
        return self._save_new_trajs_to

    @property
    def ready_for_training(self) -> bool:
        return any(d.ready_for_training for d in self.all_datasets)

    def update_dset_weights(self, epoch: int):
        self.check_configured()

        # look for newly ready datasets
        for status in self._dsets_status:
            if status.epoch_ready is None and status.dataset.ready_for_training:
                status.epoch_ready = epoch

        # set sampling weights
        self._available_datasets = []
        for status in self._dsets_status:
            if status.epoch_ready is not None:
                if self.smooth_sudden_starts is None:
                    # active datasets jump right to their final weight
                    current_weight = status.final_weight
                else:
                    # linear schedule for smooth_sudden_starts epochs
                    # after the dataset is first discovered to be ready
                    m = (
                        status.final_weight - status.initial_weight
                    ) / self.smooth_sudden_starts
                    x = epoch - status.epoch_ready + 1
                    current_weight = min(
                        m * x + status.initial_weight,
                        status.final_weight,
                    )
                self._available_datasets.append((status.dataset, current_weight))
            else:
                # if dataset is not ready for training, set weight to 0
                self._available_datasets.append((status.dataset, 0.0))

    def on_end_of_collection(self, experiment) -> dict[str, Any]:
        self.check_configured()

        # flatten the metrics from all datasets
        metrics = {}
        for d in self.all_datasets:
            d_metrics = d.on_end_of_collection(experiment)
            for k, v in d_metrics.items():
                metrics[f"{d.dset_name} {k}"] = v

        # log and then reset sampling counters
        total_samples = sum(self._sampling_metrics.values())
        if total_samples:
            while self._sampling_metrics:
                dset_name, samples = self._sampling_metrics.popitem()
                metrics[f"{dset_name} Empirical Sample Weight (Previous Epoch)"] = (
                    samples / total_samples
                )

        # find datasets that are now ready for training, and adjust their sampling weight
        self.update_dset_weights(experiment.epoch)
        # log sampling weight logic to wandb
        for dset, active_weight in self._available_datasets:
            metrics[f"{dset.dset_name} Current Sample Weight"] = active_weight
        return metrics

    def delete(self):
        for d in self.all_datasets:
            d.delete()

    def get_description(self) -> str:
        return (
            f"MixtureOfDatasets with {len(self.all_datasets)} datasets"
            + "\n"
            + "\n".join(d.get_description() for d in self.all_datasets)
        )

    def sample_random_trajectory(self) -> RLData:
        active_dsets, active_weights = zip(*self._available_datasets)
        # renormalize the weights to sum to 1.0
        total_weight = sum(active_weights)
        active_weights = [w / total_weight for w in active_weights]
        # sample from the available datasets
        random_dset = random.choices(active_dsets, weights=active_weights, k=1)[0]
        self._sampling_metrics[random_dset.dset_name] += 1
        # sample a random trajectory from the chosen dataset
        rldata = random_dset.sample_random_trajectory()
        return rldata


def load_traj_from_disk(path: str) -> Trajectory | FrozenTraj:
    """Loads a trajectory from disk.

    Switches between the available file formats of (".traj" and ".npz")

    Args:
        path: The path to the trajectory file saved by our environment wrappers as part of online data collection in `Experiment`.

    Returns:
        A :py:class:`~amago.hindsight.Trajectory` or :py:class:`~amago.hindsight.FrozenTraj` object.
    """
    _, ext = os.path.splitext(path)
    if ext == ".traj":
        with open(path, "rb") as f:
            disk = pickle.load(f)
        traj = Trajectory(timesteps=disk.timesteps)
        return traj
    elif ext == ".npz":
        disk = FrozenTraj.from_dict(np.load(path))
        return disk
    else:
        raise ValueError(
            f"Unrecognized trajectory file extension `{ext}` for path `{path}`."
        )


def get_path_to_trajs(dset_root: str, dset_name: str, fifo: bool) -> str:
    return os.path.join(dset_root, dset_name, "buffer", "fifo" if fifo else "protected")


@gin.configurable
class DiskTrajDataset(RLDataset):
    """Imitates the typical FIFO replay buffer but stores sequence data on disk.

    Tells the parallel actors to write finished trajectories to a buffer directory
    and then deletes the oldest trajectories when the dataset exceeds a max size.
    Also creates a "protected" directory where trajectory data from previous runs
    or demonstrations can be stored and sampled from without being deleted
    (in the style of DQfD and so on).

    Creates a dataset in the structure:

    .. code-block:: shell

        dset_root/
        └── dset_name/
            └── buffer/
                ├── fifo/
                │   └── # .traj or .npz files here
                └── protected/
                    └── # .traj or .npz files here

    Args:
        dset_root: The root directory to store the dataset.
        dset_name: The name of the dataset. Used to create a buffer subdirectory
            and to identify the dataset in logging metrics.
        dset_max_size: The maximum number of trajectories to keep in the FIFO buffer.
        dset_min_size: The minimum number of trajectories that need to be present
            before training updates on this dataset can begin.
        relabeler: A function that edits/relabels trajectory data when loaded.
            (`amago.hindsight.Relabeler`).
    """

    def __init__(
        self,
        dset_root: str,
        dset_name: str,
        dset_max_size: int,
        dset_min_size: int = 1,
        relabeler: Optional[Relabeler] = None,
    ):
        super().__init__(dset_name=dset_name)
        self.relabeler = NoOpRelabeler() if relabeler is None else relabeler
        # create two directories for the FIFO and protected buffers
        self.fifo_path = get_path_to_trajs(dset_root, dset_name, fifo=True)
        self.protected_path = get_path_to_trajs(dset_root, dset_name, fifo=False)
        self.dset_min_size = dset_min_size
        self.dset_max_size = dset_max_size
        os.makedirs(self.fifo_path, exist_ok=True)
        os.makedirs(self.protected_path, exist_ok=True)
        self._refresh_files()

    @property
    def save_new_trajs_to(self) -> Optional[str]:
        # tells the parallel actors where to write new trajectories
        return self.fifo_path

    @property
    def _disk_usage(self):
        bytes = sum(os.path.getsize(f) for f in self.all_filenames)
        return bytes * 1e-9

    @property
    def ready_for_training(self) -> bool:
        return (
            super().ready_for_training and len(self.all_filenames) > self.dset_min_size
        )

    def delete(self, delete_protected: bool = False):
        self.check_configured()
        # remove files on disk
        if os.path.exists(self.fifo_path):
            shutil.rmtree(self.fifo_path)
            self.fifo_filenames = set()
        if os.path.exists(self.protected_path) and delete_protected:
            shutil.rmtree(self.protected_path)
            self.protected_filenames = set()
        self.all_filenames = list(self.fifo_filenames | self.protected_filenames)

    def _list_abs_path_to_files(self, dir: str):
        names = []
        for ext in [".traj", ".npz"]:
            names.extend(
                [os.path.join(dir, f) for f in os.listdir(dir) if f.endswith(ext)]
            )
        return set(names)

    def _refresh_files(self):
        # find the new .traj files from the previous rollout
        self.fifo_filenames = self._list_abs_path_to_files(self.fifo_path)
        self.protected_filenames = self._list_abs_path_to_files(self.protected_path)
        self.all_filenames = list(self.fifo_filenames | self.protected_filenames)

    def _count_trajectories(self) -> int:
        return len(self.all_filenames)

    def _count_deletable_trajectories(self) -> int:
        return len(self.fifo_filenames)

    def _count_protected_trajectories(self) -> int:
        return len(self.protected_filenames)

    def _filter(self):
        """
        Imitates fixed-size FIFO replay buffers by clearing .traj files on disk in time order.
        """
        if len(self.fifo_filenames) <= self.dset_max_size:
            # skip the sort
            return

        path_to_name = lambda path: os.path.basename(os.path.splitext(path)[0])
        traj_infos = []
        for traj_filename in self.fifo_filenames:
            env_name, rand_id, unix_time = path_to_name(traj_filename).split("_")
            traj_infos.append(
                {
                    "env": env_name,
                    "rand": rand_id,
                    "time": float(unix_time),
                    "filename": traj_filename,
                }
            )
        traj_infos = sorted(traj_infos, key=lambda d: d["time"])
        num_to_remove = max(len(traj_infos) - self.dset_max_size, 0)
        to_delete = list(map(itemgetter("filename"), traj_infos[:num_to_remove]))
        for file_to_delete in to_delete:
            os.remove(file_to_delete)
            self.fifo_filenames.discard(file_to_delete)
        self.all_filenames = list(self.fifo_filenames | self.protected_filenames)

    def get_description(self) -> str:
        self.check_configured()
        return f"""DiskTrajDataset
        FIFO Buffer Path: {self.fifo_path}
        Protected Buffer Path: {self.protected_path}
        FIFO Buffer Max Size: {self.dset_max_size}
        FIFO Buffer Initial Size: {self._count_deletable_trajectories()}
        Protected Buffer Initial Size: {self._count_protected_trajectories()}
        Trajectory File Max Sequence Length: {self.max_seq_len}
        Trajectory Padded Sampling: {self.padded_sampling}
        """

    def on_end_of_collection(self, experiment) -> dict[str, Any]:
        """
        Implements the FIFO buffer behavior.
        """
        self._refresh_files()
        if not self.has_edit_rights:
            # this Experiment is not supposed to manage the dataset
            return {}
        # old `accelerate` paranoia
        old_size = self._count_trajectories()
        experiment.accelerator.wait_for_everyone()
        if experiment.accelerator.is_main_process:
            self._filter()
        experiment.accelerator.wait_for_everyone()
        self._refresh_files()
        dset_size = self._count_trajectories()
        fifo_size = self._count_deletable_trajectories()
        protected_size = self._count_protected_trajectories()
        return {
            "Trajectory Files Saved in FIFO Replay Buffer": fifo_size,
            "Trajectory Files Saved in Protected Replay Buffer": protected_size,
            "Total Trajectory Files in Replay Buffer": dset_size,
            "Trajectory Files Deleted": old_size - dset_size,
            "Buffer Disk Space (GB)": self._disk_usage,
        }

    def sample_random_trajectory(self) -> RLData:
        self.check_configured()
        filename = random.choice(self.all_filenames)
        traj = load_traj_from_disk(filename)
        traj = self.relabeler(traj)
        return self._traj_to_rl_data(traj)

    def _traj_to_rl_data(self, traj: Trajectory | FrozenTraj) -> RLData:
        if isinstance(traj, Trajectory):
            traj = traj.freeze()
        assert isinstance(traj, FrozenTraj)
        obs = {k: torch.from_numpy(v) for k, v in traj.obs.items()}
        rl2s = torch.from_numpy(traj.rl2s).float()
        time_idxs = torch.from_numpy(traj.time_idxs).long()
        rews = torch.from_numpy(traj.rews).float()
        dones = torch.from_numpy(traj.dones).bool()
        actions = torch.from_numpy(traj.actions).float()
        return RLData(
            obs=obs,
            rl2s=rl2s,
            time_idxs=time_idxs,
            rews=rews,
            dones=dones,
            actions=actions,
        )


class DoNothingDataset(RLDataset):
    """A dataset that does nothing and is never ready for training.

    Useful for repurposing Experiment to manage inference or eval only.
    """

    def __init__(self):
        super().__init__(dset_name="DoNothingDataset")

    @property
    def save_new_trajs_to(self):
        return None

    @property
    def ready_for_training(self):
        return False

    def get_description(self):
        return self.dset_name

    def sample_random_trajectory(self) -> RLData:
        raise NotImplementedError("DoNothingDataset does not support training!")
